{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# corpus bootstrapping with LM perplexity\n",
    "\n",
    "toy test of Ramaswamy, Printz, Gopalakrishnan: *A Bootstrap Technique for Building Domain-Dependent Language Models*\n",
    "http://mirlab.org/conference_papers/International_Conference/ICSLP%201998/PDF/SCAN/SL980611.PDF\n",
    "\n",
    "- uses simple bigram LM with add-k smoothing\n",
    "- in-domain data from Jane Austen\n",
    "- 'unlabeled' corpus of Austen,Carroll and Melville sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "import nltk\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# in-domain corpus data\n",
    "\n",
    "load the three jane austen texts as tokenized sentences, preprocess (lowercase, remove punctuation etc, add `<s>` and `</s>` tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16498\n"
     ]
    }
   ],
   "source": [
    "# read corpora\n",
    "austen1 = nltk.corpus.gutenberg.sents('austen-emma.txt')\n",
    "austen2 = nltk.corpus.gutenberg.sents('austen-persuasion.txt')\n",
    "austen3 = nltk.corpus.gutenberg.sents('austen-sense.txt')\n",
    "data = austen1 + austen2 + austen3\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4124 12374\n",
      "CPU times: user 5.99 s, sys: 96 ms, total: 6.08 s\n",
      "Wall time: 6.09 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# shuffle data and withhold random set\n",
    "indices = [i for i in range(len(data))]\n",
    "random.shuffle(indices)\n",
    "data = [data[i] for i in indices]\n",
    "\n",
    "test_idx = int(len(data)*0.25)\n",
    "corpus = data[:test_idx]\n",
    "withheld = data[test_idx:]\n",
    "data = None # clear\n",
    "print(len(corpus), len(withheld))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing\n",
    "\n",
    "remove sents of len < 5 (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 4.53 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# preprocess data (with function)\n",
    "def preprocess(tokens):\n",
    "    processed = []\n",
    "    for sent in tokens:\n",
    "        if len(sent) > 6:\n",
    "            this_sent = []\n",
    "            for word in sent:\n",
    "                if re.findall(r'[0-9A-Za-z]+', word):\n",
    "                    this_sent.append(word.lower())\n",
    "            this_sent = ['<s>'] + this_sent + ['</s>']\n",
    "            processed.append(this_sent)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed corpus: 3627 withheld: 10868\n"
     ]
    }
   ],
   "source": [
    "corpus = preprocess(corpus)\n",
    "withheld = preprocess(withheld)\n",
    "print(\"seed corpus:\", len(corpus), \"withheld:\", len(withheld))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build bigram language model\n",
    "\n",
    "we will constuct this as a function so we can iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def languagemodel(corpus):\n",
    "    \n",
    "    # bigramize and get vocabulary\n",
    "    bigrams = []\n",
    "    vocab = []\n",
    "    for sent in corpus:\n",
    "        # split\n",
    "        for i in range(len(sent)-1):\n",
    "            vocab.append(sent[i])\n",
    "            bigrams.append((sent[i], sent[i+1]))\n",
    "        vocab.append(sent[-1])\n",
    "        \n",
    "    # get vocabulary size, counters\n",
    "    # oovs will be handled by smoothing\n",
    "    vocabsize = len(set(vocab))\n",
    "    vocab_counts = Counter(vocab)\n",
    "    bigram_counts = Counter(bigrams)\n",
    "    \n",
    "    return vocabsize, vocab_counts, bigram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 76 ms, sys: 4 ms, total: 80 ms\n",
      "Wall time: 81.9 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocabsize, vocab_counts, bigram_counts = languagemodel(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# perplexity measurement\n",
    "\n",
    "this is a function of the probability of a given sentence under this language model's assumptions (i.e the product of the probability of each bigram normalized by the total sentence length)\n",
    "\n",
    "we use add-k smoothing to account for out-of-vocab terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# perplexity calculation with add-k smoothing\n",
    "# from http://www3.cs.stonybrook.edu/~has/CSE594/Notes/(1)%20Probability%20Theory%20Application%201-28.pdf\n",
    "def perplexity(sent, k=0.1):\n",
    "    # tokenize and bigramize\n",
    "    bigrams = []\n",
    "    for i in range(len(sent)-1):\n",
    "        bigrams.append((sent[i], sent[i+1]))\n",
    "        \n",
    "    # for each bigram, get probability (add-one smoothed)\n",
    "    # p(w_i | w_i-1) = count(w_i-1, w_i) + 1 / count(w_i-1 + N)\n",
    "    probs = []\n",
    "    for bigram in bigrams:\n",
    "        b_c = bigram_counts[bigram] + k\n",
    "        w_c = vocab_counts[bigram[0]] + k*vocabsize\n",
    "        probs.append(b_c/w_c)\n",
    "    \n",
    "    # get inverse\n",
    "    inverse = [1.0/prob for prob in probs]\n",
    "    \n",
    "    # get product\n",
    "    product = 1.0\n",
    "    for inv in inverse:\n",
    "        product *= inv\n",
    "    \n",
    "    # n-th root\n",
    "    perplexity = math.pow(product, (1/len(sent)))\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# external corpus\n",
    "\n",
    "this is data from an external source that (hopefully) includes some sentences that we can use for data augmentation.\n",
    "\n",
    "here we (artificially) create a mixed id/ood corpus by mixing our withheld data in with some text from another source. we will use moby dick because it is one of the NLTK prose texts closer in time to Jane Austen\n",
    "\n",
    "for testing, we will label each sentence according to source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10868"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "withheld = [('austen', s) for s in withheld]\n",
    "len(withheld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8213, 1360)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melville = nltk.corpus.gutenberg.sents('melville-moby_dick.txt')\n",
    "melville = preprocess(melville)\n",
    "melville = [('melville', s) for s in melville]\n",
    "\n",
    "carroll = nltk.corpus.gutenberg.sents('carroll-alice.txt')\n",
    "carroll = preprocess(carroll)\n",
    "carroll = [('carroll', s) for s in carroll]\n",
    "\n",
    "len(melville), len(carroll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20441"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled = withheld + melville + carroll\n",
    "len(unlabeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test: sort by perplexity score\n",
    "\n",
    "as we can see, in-domain answers are at the top. of course it is not the case that necessarily all *(true)* in-domain sentences are at the top of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 456 ms, sys: 0 ns, total: 456 ms\n",
      "Wall time: 458 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get perplexities\n",
    "perplexities = [perplexity(s[1]) for s in unlabeled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('austen', '<s> i have no doubt of it </s>', 19.26374434915156),\n",
       " ('melville', '<s> i am sure that i did not </s>', 20.082746876015545),\n",
       " ('austen', '<s> i am glad of it </s>', 22.101978371246553),\n",
       " ('austen', '<s> i am said he </s>', 23.184076077461448),\n",
       " ('austen', '<s> i do not know my dear </s>', 23.66308393573234),\n",
       " ('austen', '<s> i thought it must be so </s>', 25.546392898905058),\n",
       " ('austen', '<s> i have had such a time of it </s>', 25.665720298705054),\n",
       " ('austen', '<s> to be sure i am </s>', 26.431308081000193),\n",
       " ('austen', '<s> i am sure i do not know who is not </s>', 28.13262260573392),\n",
       " ('austen', '<s> i did not know what to do </s>', 28.276810677949708),\n",
       " ('austen', '<s> i do not think they were </s>', 28.90714748715335),\n",
       " ('austen', '<s> i could not have believed it </s>', 29.298706291228427),\n",
       " ('austen', '<s> i do not know what he would be at </s>', 30.706242045240288),\n",
       " ('austen', '<s> he had no idea of me </s>', 32.324620768227796),\n",
       " ('austen', '<s> i assure you i shall not </s>', 32.45905493971622),\n",
       " ('austen', '<s> i do not pretend to it </s>', 32.77845876593376),\n",
       " ('austen', '<s> i dare say she had </s>', 33.08791976724152),\n",
       " ('melville', '<s> you must have heard of it </s>', 33.72388746354324),\n",
       " ('austen', '<s> yes he had done it </s>', 34.072475737126254),\n",
       " ('austen',\n",
       "  '<s> i am sure you must have heard of her before </s>',\n",
       "  34.07346365425782)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort by perplexity (lower = better)\n",
    "[(x[0], ' '.join(x[1]), y) for x, y in sorted(zip(unlabeled, perplexities), key=lambda pair: pair[1])][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iterate\n",
    "\n",
    "this is meant to be an iterative algorithm, so we add the top sentences (using threshold) to the original training data, make a new language model, and calculate new perplexity scores over the outside data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10 : total added 275 sents\n",
      "iter 20 : total added 525 sents\n",
      "iter 30 : total added 775 sents\n",
      "iter 40 : total added 1025 sents\n",
      "iter 50 : total added 1275 sents\n",
      "iter 60 : total added 1525 sents\n",
      "no added sentences, stopping...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iters = 500\n",
    "\n",
    "add_corpus = corpus[:]       # the expanding id-corpus\n",
    "rem_unlabeled = unlabeled[:] # the shrinking unlabeled data\n",
    "additions = []               # track additions to lm corpus\n",
    "threshhold = 100.0           # perplexity threshhold\n",
    "cutoff = 25                  # cutoff for added sents, make large to 'ignore'\n",
    "k = 0.001                    # smoothing term for perplexity add-k smoothing\n",
    "\n",
    "for i in range(iters):\n",
    "    \n",
    "    # EarlyStopping\n",
    "    cnt = 0\n",
    "    \n",
    "    # indices to remove from unlabeled data\n",
    "    remove_idx = []\n",
    "    \n",
    "    # build language model\n",
    "    vocabsize, vocab_counts, bigram_counts = languagemodel(add_corpus)\n",
    "    \n",
    "    # get perplexities\n",
    "    perplexities = [perplexity(s[1], k) for s in rem_unlabeled]\n",
    "    \n",
    "    # indices, sort perplexities\n",
    "    indices = [i for i in range(len(perplexities))]\n",
    "    sorted_perplexities = [(x, y) for x, y in sorted(zip(indices, perplexities), key=lambda pair: pair[1])]\n",
    "    \n",
    "    # take top sents\n",
    "    add = 0\n",
    "    for jdx, tup in enumerate(sorted_perplexities):\n",
    "        idx = tup[0]\n",
    "        perp = tup[1]\n",
    "        if perp < threshhold:\n",
    "            additions.append(rem_unlabeled[idx])\n",
    "            add_corpus.append(rem_unlabeled[idx][1])\n",
    "            remove_idx.append(idx)\n",
    "            cnt += 1\n",
    "            add += 1\n",
    "        if add == cutoff:\n",
    "            break\n",
    "    \n",
    "    # filter out additions\n",
    "    rem_unlabeled = [rem_unlabeled[i] for i in range(len(rem_unlabeled)) if i not in remove_idx]\n",
    "    \n",
    "    # if no added sents, terminate\n",
    "    if cnt == 0:\n",
    "        print(\"no added sentences, stopping...\\n\")\n",
    "#         debug = [(x[0], ' '.join(x[1]), y) for x, y in sorted(zip(rem_unlabeled, perplexities), key=lambda pair: pair[1])][:100]\n",
    "#         for d in debug[:10]:\n",
    "#             print(d)\n",
    "        break\n",
    "    \n",
    "    if i > 0 and i % 10 == 0:\n",
    "        print(\"iter\", i, \": total added\", len(additions), \"sents\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sents found: 1619 (% 7.920356146959542 of unlabeled)\n"
     ]
    }
   ],
   "source": [
    "diff = len(add_corpus) - len(corpus)\n",
    "totl = len(unlabeled)\n",
    "print(\"sents found:\", diff, \"(%\", diff*100/totl, \"of unlabeled)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision of found sents:  0.9246448424953675\n"
     ]
    }
   ],
   "source": [
    "labels = [t[0] for t in additions]\n",
    "corrects = [t[0] for t in additions if t[0]=='austen']\n",
    "print(\"precision of found sents: \", len(corrects)/len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of trues in unlabeled: 0.531676532459273\n"
     ]
    }
   ],
   "source": [
    "punlabeled = len(withheld)/len(withheld + melville + carroll)\n",
    "print(\"percentage of trues in unlabeled:\", punlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall of unlabeled austen sents:  0.13774383511225616\n"
     ]
    }
   ],
   "source": [
    "recall = len(corrects)/len(withheld)\n",
    "print(\"recall of unlabeled austen sents: \", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "core_eng",
   "language": "python",
   "name": "core_eng"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
