{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# corpus bootstrapping with LM perplexity\n",
    "\n",
    "toy test of Ramaswamy, Printz, Gopalakrishnan: *A Bootstrap Technique for Building Domain-Dependent Langauge Models*\n",
    "http://mirlab.org/conference_papers/International_Conference/ICSLP%201998/PDF/SCAN/SL980611.PDF\n",
    "\n",
    "- uses simple bigram LM with add-k smoothing\n",
    "- in-domain data from Jane Austen\n",
    "- 'unlabeled' corpus of Austen,Carroll and Melville sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "import nltk\n",
    "import random\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# in-domain corpus data\n",
    "\n",
    "load the three jane austen texts as tokenized sentences, preprocess (lowercase, remove punctuation etc, add `<s>` and `</s>` tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16498\n"
     ]
    }
   ],
   "source": [
    "# read corpora\n",
    "austen1 = nltk.corpus.gutenberg.sents('austen-emma.txt')\n",
    "austen2 = nltk.corpus.gutenberg.sents('austen-persuasion.txt')\n",
    "austen3 = nltk.corpus.gutenberg.sents('austen-sense.txt')\n",
    "data = austen1 + austen2 + austen3\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4124 12374\n",
      "CPU times: user 5.85 s, sys: 136 ms, total: 5.98 s\n",
      "Wall time: 5.99 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# shuffle data and withhold random set\n",
    "indices = [i for i in range(len(data))]\n",
    "random.shuffle(indices)\n",
    "data = [data[i] for i in indices]\n",
    "\n",
    "test_idx = int(len(data)*0.25)\n",
    "corpus = data[:test_idx]\n",
    "withheld = data[test_idx:]\n",
    "data = None # clear\n",
    "print(len(corpus), len(withheld))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing\n",
    "\n",
    "remove sents of len < 5 (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 5.96 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# preprocess data (with function)\n",
    "def preprocess(tokens):\n",
    "    processed = []\n",
    "    for sent in tokens:\n",
    "        if len(sent) > 4:\n",
    "            this_sent = []\n",
    "            for word in sent:\n",
    "                if re.findall(r'[0-9A-Za-z]+', word):\n",
    "                    this_sent.append(word.lower())\n",
    "            this_sent = ['<s>'] + this_sent + ['</s>']\n",
    "            processed.append(this_sent)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed corpus: 3870 withheld: 11588\n"
     ]
    }
   ],
   "source": [
    "corpus = preprocess(corpus)\n",
    "withheld = preprocess(withheld)\n",
    "print(\"seed corpus:\", len(corpus), \"withheld:\", len(withheld))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build bigram language model\n",
    "\n",
    "we will constuct this as a function so we can iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def languagemodel(corpus):\n",
    "    \n",
    "    # bigramize and get vocabulary\n",
    "    bigrams = []\n",
    "    vocab = []\n",
    "    for sent in corpus:\n",
    "        # split\n",
    "        for i in range(len(sent)-1):\n",
    "            vocab.append(sent[i])\n",
    "            bigrams.append((sent[i], sent[i+1]))\n",
    "        vocab.append(sent[-1])\n",
    "        \n",
    "    # get vocabulary size, counters\n",
    "    # oovs will be handled by smoothing\n",
    "    vocabsize = len(set(vocab))\n",
    "    vocab_counts = Counter(vocab)\n",
    "    bigram_counts = Counter(bigrams)\n",
    "    \n",
    "    return vocabsize, vocab_counts, bigram_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 76 ms, sys: 4 ms, total: 80 ms\n",
      "Wall time: 79.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "vocabsize, vocab_counts, bigram_counts = languagemodel(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# perplexity measurement\n",
    "\n",
    "this is a function of the probability of a given sentence under this language model's assumptions (i.e the product of the probability of each bigram normalized by the total sentence length)\n",
    "\n",
    "we use add-k smoothing to account for out-of-vocab terms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# perplexity calculation with add-k smoothing\n",
    "# from http://www3.cs.stonybrook.edu/~has/CSE594/Notes/(1)%20Probability%20Theory%20Application%201-28.pdf\n",
    "def perplexity(sent, k=0.1):\n",
    "    # tokenize and bigramize\n",
    "    bigrams = []\n",
    "    for i in range(len(sent)-1):\n",
    "        bigrams.append((sent[i], sent[i+1]))\n",
    "        \n",
    "    # for each bigram, get probability (add-one smoothed)\n",
    "    # p(w_i | w_i-1) = count(w_i-1, w_i) + 1 / count(w_i-1 + N)\n",
    "    probs = []\n",
    "    for bigram in bigrams:\n",
    "        b_c = bigram_counts[bigram] + k\n",
    "        w_c = vocab_counts[bigram[0]] + k*vocabsize\n",
    "        probs.append(b_c/w_c)\n",
    "    \n",
    "    # get inverse\n",
    "    inverse = [1.0/prob for prob in probs]\n",
    "    \n",
    "    # get product\n",
    "    product = 1.0\n",
    "    for inv in inverse:\n",
    "        product *= inv\n",
    "    \n",
    "    # n-th root\n",
    "    perplexity = math.pow(product, (1/len(sent)))\n",
    "    \n",
    "    return perplexity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# external corpus\n",
    "\n",
    "this is data from an external source that (hopefully) includes some sentences that we can use for data augmentation.\n",
    "\n",
    "here we (artificially) create a mixed id/ood corpus by mixing our withheld data in with some text from another source. we will use moby dick because it is one of the NLTK prose texts closer in time to Jane Austen\n",
    "\n",
    "for testing, we will label each sentence according to source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11588"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "withheld = [('austen', s) for s in withheld]\n",
    "len(withheld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8857, 1514)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melville = nltk.corpus.gutenberg.sents('melville-moby_dick.txt')\n",
    "melville = preprocess(melville)\n",
    "melville = [('melville', s) for s in melville]\n",
    "\n",
    "carroll = nltk.corpus.gutenberg.sents('carroll-alice.txt')\n",
    "carroll = preprocess(carroll)\n",
    "carroll = [('carroll', s) for s in carroll]\n",
    "\n",
    "len(melville), len(carroll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21959"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled = withheld + melville + carroll\n",
    "len(unlabeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test: sort by perplexity score\n",
    "\n",
    "as we can see, in-domain answers are at the top. of course it is not the case that necessarily all *(true)* in-domain sentences are at the top of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 448 ms, sys: 0 ns, total: 448 ms\n",
      "Wall time: 449 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get perplexities\n",
    "perplexities = [perplexity(s[1]) for s in unlabeled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('austen', '<s> i do not know </s>', 14.402780014643374),\n",
       " ('austen', '<s> i do not know </s>', 14.402780014643374),\n",
       " ('austen', '<s> i do not know </s>', 14.402780014643374),\n",
       " ('austen', '<s> i am sure i do not know </s>', 17.073304545881072),\n",
       " ('carroll', '<s> i had not </s>', 18.467994755109974),\n",
       " ('austen', '<s> i think it is so </s>', 19.13642298145079),\n",
       " ('austen', '<s> she had seen him </s>', 19.373915146348256),\n",
       " ('austen', '<s> she could not speak </s>', 19.929227200171315),\n",
       " ('melville', '<s> i am sure that i did not </s>', 20.070980673944522),\n",
       " ('austen', '<s> i am very happy </s>', 20.34386028804892),\n",
       " ('austen', '<s> i am sure i had not </s>', 20.372056832569804),\n",
       " ('austen', '<s> it must be so </s>', 20.63276115419619),\n",
       " ('melville', '<s> i will tell you </s>', 21.74843489942217),\n",
       " ('melville', '<s> but it is not so </s>', 21.83250613705818),\n",
       " ('austen', '<s> i do not understand it </s>', 22.094802382935576),\n",
       " ('austen', '<s> i am glad of it </s>', 22.41667777778088),\n",
       " ('austen', '<s> i am sure we do </s>', 22.727148525903782),\n",
       " ('melville', '<s> i do as i do </s>', 22.918128797664025),\n",
       " ('austen', '<s> it is too much </s>', 24.21462131885731),\n",
       " ('austen', '<s> i thought it must be so </s>', 24.49577440163642)]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort by perplexity (lower = better)\n",
    "[(x[0], ' '.join(x[1]), y) for x, y in sorted(zip(unlabeled, perplexities), key=lambda pair: pair[1])][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iterate\n",
    "\n",
    "this is meant to be an iterative algorithm, so we add the top sentences (using threshold) to the original training data, make a new language model, and calculate new perplexity scores over the outside data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10 : total added 275 sents\n",
      "iter 20 : total added 525 sents\n",
      "iter 30 : total added 775 sents\n",
      "iter 40 : total added 1025 sents\n",
      "iter 50 : total added 1275 sents\n",
      "iter 60 : total added 1525 sents\n",
      "iter 70 : total added 1775 sents\n",
      "iter 80 : total added 2025 sents\n",
      "no added sentences, stopping...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iters = 500\n",
    "\n",
    "add_corpus = corpus[:]       # the expanding id-corpus\n",
    "rem_unlabeled = unlabeled[:] # the shrinking unlabeled data\n",
    "additions = []               # track additions to lm corpus\n",
    "threshhold = 100.0           # perplexity threshhold\n",
    "cutoff = 25                  # cutoff for added sents, make large to 'ignore'\n",
    "k = 0.001                    # smoothing term for perplexity add-k smoothing\n",
    "\n",
    "for i in range(iters):\n",
    "    \n",
    "    # EarlyStopping\n",
    "    cnt = 0\n",
    "    \n",
    "    # indices to remove from unlabeled data\n",
    "    remove_idx = []\n",
    "    \n",
    "    # build language model\n",
    "    vocabsize, vocab_counts, bigram_counts = languagemodel(add_corpus)\n",
    "    \n",
    "    # rank perplexities\n",
    "    perplexities = [perplexity(s[1], k) for s in rem_unlabeled]\n",
    "    \n",
    "    # take top sents\n",
    "    add = 0\n",
    "    for idx, perp in enumerate(perplexities):\n",
    "        if perp < threshhold:\n",
    "            additions.append(rem_unlabeled[idx])\n",
    "            add_corpus.append(rem_unlabeled[idx][1])\n",
    "            remove_idx.append(idx)\n",
    "            cnt += 1\n",
    "            add += 1\n",
    "        if add == cutoff:\n",
    "            break\n",
    "    \n",
    "    # filter out additions\n",
    "    rem_unlabeled = [rem_unlabeled[i] for i in range(len(rem_unlabeled)) if i not in remove_idx]\n",
    "    \n",
    "    # if no added sents, terminate\n",
    "    if cnt == 0:\n",
    "        print(\"no added sentences, stopping...\\n\")\n",
    "#         debug = [(x[0], ' '.join(x[1]), y) for x, y in sorted(zip(rem_unlabeled, perplexities), key=lambda pair: pair[1])][:100]\n",
    "#         for d in debug[:10]:\n",
    "#             print(d)\n",
    "        break\n",
    "    \n",
    "    if i > 0 and i % 10 == 0:\n",
    "        print(\"iter\", i, \": total added\", len(additions), \"sents\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sents found: 2162 (% 9.845621385308984 of unlabeled)\n"
     ]
    }
   ],
   "source": [
    "diff = len(add_corpus) - len(corpus)\n",
    "totl = len(unlabeled)\n",
    "print(\"sents found:\", diff, \"(%\", diff*100/totl, \"of unlabeled)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision of found sents:  0.8973172987974098\n"
     ]
    }
   ],
   "source": [
    "labels = [t[0] for t in additions]\n",
    "corrects = [t[0] for t in additions if t[0]=='austen']\n",
    "print(\"precision of found sents: \", len(corrects)/len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of trues in unlabeled: 0.5277107336399653\n"
     ]
    }
   ],
   "source": [
    "punlabeled = len(withheld)/len(withheld + melville + carroll)\n",
    "print(\"percentage of trues in unlabeled:\", punlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall of unlabeled austen sents:  0.16741456679323438\n"
     ]
    }
   ],
   "source": [
    "recall = len(corrects)/len(withheld)\n",
    "print(\"recall of unlabeled austen sents: \", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "core_eng",
   "language": "python",
   "name": "core_eng"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
