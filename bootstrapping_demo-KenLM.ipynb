{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# corpus bootstrapping with LM perplexity\n",
    "\n",
    "toy test of Ramaswamy, Printz, Gopalakrishnan: *A Bootstrap Technique for Building Domain-Dependent Langauge Models*\n",
    "http://mirlab.org/conference_papers/International_Conference/ICSLP%201998/PDF/SCAN/SL980611.PDF\n",
    "\n",
    "- uses KenLM n-gram LM with backoff and smoothing\n",
    "- in-domain data from Jane Austen\n",
    "- 'unlabeled' corpus of Austen,Carroll and Melville sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import kenlm\n",
    "import math\n",
    "import nltk\n",
    "import random\n",
    "import re\n",
    "import subprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# in-domain corpus data\n",
    "\n",
    "load the three jane austen texts as tokenized sentences, preprocess (lowercase, remove punctuation etc, add `<s>` and `</s>` tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16498\n"
     ]
    }
   ],
   "source": [
    "# read corpora\n",
    "austen1 = nltk.corpus.gutenberg.sents('austen-emma.txt')\n",
    "austen2 = nltk.corpus.gutenberg.sents('austen-persuasion.txt')\n",
    "austen3 = nltk.corpus.gutenberg.sents('austen-sense.txt')\n",
    "data = austen1 + austen2 + austen3\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4124 12374\n",
      "CPU times: user 6.06 s, sys: 100 ms, total: 6.16 s\n",
      "Wall time: 6.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# shuffle data and withhold random set\n",
    "indices = [i for i in range(len(data))]\n",
    "random.shuffle(indices)\n",
    "data = [data[i] for i in indices]\n",
    "\n",
    "test_idx = int(len(data)*0.25)\n",
    "corpus = data[:test_idx]\n",
    "withheld = data[test_idx:]\n",
    "data = None # clear\n",
    "print(len(corpus), len(withheld))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## preprocessing\n",
    "\n",
    "remove sents of len < 5 (words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 0 ns, sys: 0 ns, total: 0 ns\n",
      "Wall time: 4.05 Âµs\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# preprocess data (with function)\n",
    "def preprocess(tokens):\n",
    "    processed = []\n",
    "    for sent in tokens:\n",
    "        if len(sent) > 6:\n",
    "            this_sent = []\n",
    "            for word in sent:\n",
    "                if re.findall(r'[0-9A-Za-z]+', word):\n",
    "                    this_sent.append(word.lower())\n",
    "            processed.append(this_sent)\n",
    "    return processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seed corpus: 3630 withheld: 10865\n"
     ]
    }
   ],
   "source": [
    "corpus = preprocess(corpus)\n",
    "withheld = preprocess(withheld)\n",
    "print(\"seed corpus:\", len(corpus), \"withheld:\", len(withheld))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write corpus file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save the text file for training\n",
    "def writecorpus(lol, filename='corpus.txt'):\n",
    "    with open(filename, 'w') as f:\n",
    "        for line in corpus:\n",
    "            f.write(' '.join(line))\n",
    "            f.write('\\n')\n",
    "            \n",
    "    subprocess.run([\"bzip2\", filename])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "writecorpus(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# functions for language model\n",
    "\n",
    "we will constuct this as a function so we can iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# setup paths\n",
    "corpus_path = '/home/derek/PycharmProjects/perplexitybootstrapping/'\n",
    "process_path = '/home/derek/PycharmProjects/perplexitybootstrapping/'\n",
    "kenlm_path = '/home/derek/kcrong_stuff/kenlm/build/bin/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save bashscript\n",
    "def trainmodel(filename=\"corpus\"):\n",
    "    \n",
    "    with open(\"train.sh\", \"w\") as f:\n",
    "        f.write\n",
    "        f.write(\"bzcat \" + filename + \".txt.bz2 | python process.py | \" + kenlm_path + \"lmplz -o 3 > \"+filename+\".arpa\")\n",
    "    \n",
    "    subprocess.run([\"sh\", \"train.sh\"])\n",
    "    \n",
    "    subprocess.run([kenlm_path + \"build_binary\", filename+\".arpa\", filename+\".klm\"])\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test\n",
    "trainmodel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# external corpus\n",
    "\n",
    "this is data from an external source that (hopefully) includes some sentences that we can use for data augmentation.\n",
    "\n",
    "here we (artificially) create a mixed id/ood corpus by mixing our withheld data in with some text from another source. we will use moby dick because it is one of the NLTK prose texts closer in time to Jane Austen\n",
    "\n",
    "for testing, we will label each sentence according to source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10865"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "withheld = [('austen', ' '.join(s)) for s in withheld]\n",
    "len(withheld)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8213, 1360)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "melville = nltk.corpus.gutenberg.sents('melville-moby_dick.txt')\n",
    "melville = preprocess(melville)\n",
    "melville = [('melville', ' '.join(s)) for s in melville]\n",
    "\n",
    "carroll = nltk.corpus.gutenberg.sents('carroll-alice.txt')\n",
    "carroll = preprocess(carroll)\n",
    "carroll = [('carroll', ' '.join(s)) for s in carroll]\n",
    "\n",
    "len(melville), len(carroll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20438"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled = withheld + melville + carroll\n",
    "len(unlabeled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test: sort by perplexity score\n",
    "\n",
    "as we can see, in-domain answers are at the top. of course it is not the case that necessarily all *(true)* in-domain sentences are at the top of the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = kenlm.LanguageModel('corpus.klm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.30402205493549"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.perplexity('hello i do not know')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('austen',\n",
       " 'sir walter spurned the idea of its being offered in any manner forbad the slightest hint being dropped of his having such an intention and it was only on the supposition of his being spontaneously solicited by some most unexceptionable applicant on his own terms and as a great favour that he would let it at all')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unlabeled[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 100 ms, sys: 0 ns, total: 100 ms\n",
      "Wall time: 100 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# get perplexities\n",
    "perplexities = [model.perplexity(s[1]) for s in unlabeled]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('austen', 6.290551935861828),\n",
       " ('austen', 6.367989251454348),\n",
       " ('austen', 7.723299442582442),\n",
       " ('austen', 8.21149126462013),\n",
       " ('austen', 8.37518090089801),\n",
       " ('austen', 8.535094078534003),\n",
       " ('austen', 8.545149411874746),\n",
       " ('austen', 8.683366250438047),\n",
       " ('austen', 8.720529525773717),\n",
       " ('austen', 9.058097103575982),\n",
       " ('austen', 9.162683697748044),\n",
       " ('austen', 9.187853875959597),\n",
       " ('melville', 9.320279750239695),\n",
       " ('austen', 9.4382038615718),\n",
       " ('austen', 9.611676219540023),\n",
       " ('austen', 9.668542114475946),\n",
       " ('austen', 9.817523335443656),\n",
       " ('austen', 9.829872697923616),\n",
       " ('austen', 9.873216842188617),\n",
       " ('austen', 9.961176180150904)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sort by perplexity (lower = better)\n",
    "[(x[0], y) for x, y in sorted(zip(unlabeled, perplexities), key=lambda pair: pair[1])][:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# iterate\n",
    "\n",
    "this is meant to be an iterative algorithm, so we add the top sentences (using threshold) to the original training data, make a new language model, and calculate new perplexity scores over the outside data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 10 : total added 220 sents\n",
      "iter 20 : total added 420 sents\n",
      "iter 30 : total added 620 sents\n",
      "iter 40 : total added 820 sents\n",
      "iter 50 : total added 1020 sents\n",
      "iter 60 : total added 1220 sents\n",
      "iter 70 : total added 1420 sents\n",
      "iter 80 : total added 1620 sents\n",
      "iter 90 : total added 1820 sents\n",
      "iter 100 : total added 2020 sents\n",
      "iter 110 : total added 2220 sents\n",
      "iter 120 : total added 2420 sents\n",
      "iter 130 : total added 2620 sents\n",
      "iter 140 : total added 2820 sents\n",
      "iter 150 : total added 3020 sents\n",
      "no added sentences, stopping...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "iters = 500\n",
    "\n",
    "add_corpus = corpus[:]       # the expanding id-corpus\n",
    "rem_unlabeled = unlabeled[:] # the shrinking unlabeled data\n",
    "additions = []               # track additions to lm corpus\n",
    "threshhold = 50.0            # perplexity threshhold\n",
    "cutoff = 20                  # cutoff for added sents, make large to 'ignore'\n",
    "\n",
    "for i in range(iters):\n",
    "    \n",
    "    # EarlyStopping\n",
    "    cnt = 0\n",
    "    \n",
    "    # indices to remove from unlabeled data\n",
    "    remove_idx = []\n",
    "    \n",
    "    # write corpus\n",
    "    writecorpus(add_corpus, filename='addcorpus.txt')\n",
    "    \n",
    "    # build language model\n",
    "    trainmodel(filename='addcorpus')\n",
    "    \n",
    "    # load model\n",
    "    model = kenlm.LanguageModel('addcorpus.klm')\n",
    "    \n",
    "    # get perplexities\n",
    "    perplexities = [model.perplexity(s[1]) for s in rem_unlabeled]\n",
    "    \n",
    "    # indices, sort perplexities\n",
    "    indices = [i for i in range(len(perplexities))]\n",
    "    sorted_perplexities = [(x, y) for x, y in sorted(zip(indices, perplexities), key=lambda pair: pair[1])]\n",
    "    \n",
    "    # take top sents\n",
    "    add = 0\n",
    "    for jdx, tup in enumerate(sorted_perplexities):\n",
    "        idx = tup[0]\n",
    "        perp = tup[1]\n",
    "        if perp < threshhold:\n",
    "            additions.append(rem_unlabeled[idx])\n",
    "            add_corpus.append(rem_unlabeled[idx][1])\n",
    "            remove_idx.append(idx)\n",
    "            cnt += 1\n",
    "            add += 1\n",
    "        if add == cutoff:\n",
    "            break\n",
    "    \n",
    "    # filter out additions\n",
    "    rem_unlabeled = [rem_unlabeled[i] for i in range(len(rem_unlabeled)) if i not in remove_idx]\n",
    "    \n",
    "    # if no added sents, terminate\n",
    "    if cnt == 0:\n",
    "        print(\"no added sentences, stopping...\\n\")\n",
    "#         debug = [(x[0], ' '.join(x[1]), y) for x, y in sorted(zip(rem_unlabeled, perplexities), key=lambda pair: pair[1])][:100]\n",
    "#         for d in debug[:10]:\n",
    "#             print(d)\n",
    "        break\n",
    "    \n",
    "    if i > 0 and i % 10 == 0:\n",
    "        print(\"iter\", i, \": total added\", len(additions), \"sents\")\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sents found: 3130 (% 15.314610040121343 of unlabeled)\n"
     ]
    }
   ],
   "source": [
    "diff = len(add_corpus) - len(corpus)\n",
    "totl = len(unlabeled)\n",
    "print(\"sents found:\", diff, \"(%\", diff*100/totl, \"of unlabeled)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision of found sents:  0.9913738019169329\n"
     ]
    }
   ],
   "source": [
    "labels = [t[0] for t in additions]\n",
    "corrects = [t[0] for t in additions if t[0]=='austen']\n",
    "print(\"precision of found sents: \", len(corrects)/len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "percentage of trues in unlabeled: 0.5316077894118798\n"
     ]
    }
   ],
   "source": [
    "punlabeled = len(withheld)/len(withheld + melville + carroll)\n",
    "print(\"percentage of trues in unlabeled:\", punlabeled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "recall of unlabeled austen sents:  0.28559595029912566\n"
     ]
    }
   ],
   "source": [
    "recall = len(corrects)/len(withheld)\n",
    "print(\"recall of unlabeled austen sents: \", recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "core_eng",
   "language": "python",
   "name": "core_eng"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
